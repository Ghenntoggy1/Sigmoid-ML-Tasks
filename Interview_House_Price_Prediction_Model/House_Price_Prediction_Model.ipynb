{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Interview Task: House Price Prediction Model\n",
    "\n",
    "For the start, I was given the dataset of house prices alongside with different features that can influence the price of a house.\n",
    "\n",
    "The task is to build and train a model that will predict the price of a house based on a set of given features.\n",
    "\n",
    "First step is to take a look at the given dataset, understand them and try to make some assumptions based on previous real-life experience and knowledge received from a brief look of the data I have."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "cell_type": "code",
   "id": "9ec4ca74aca0a735",
   "metadata": {},
   "source": [
    "# Import of the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "# plt.style.use('seaborn-v0_8-muted')\n",
    "# plt.style.use({'figure.facecolor': 'white'})\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d07fccc2900bc4a",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset\n",
    "For this step, I downloaded the dataset from Kagglehub (https://www.kaggle.com/ahmednour/house-price-prediction). In order to efficiently read the dataset, I used the pandas library, specifically the method \"read_csv()\", that will read the contents of the dataset and store it in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "source": "dataset = pd.read_csv('./dataset/house_price_dataset.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6716c672a5d0c73",
   "metadata": {},
   "source": [
    "## Step 2: Display the dataset\n",
    "\n",
    "After I read the CSV dataset, I need to have some insights into what data is contained inside of it. For that purpose, I will use dataframes extensive methods to display the dataset, its description and useful information about it."
   ]
  },
  {
   "cell_type": "code",
   "id": "b3f68222946e2f25",
   "metadata": {},
   "source": [
    "print(\"Step 2: Display the dataset (first 10 rows and last 10 rows)\")\n",
    "\n",
    "\n",
    "# display - method from IPython library that will display the given argument in a more pleasant way\n",
    "display(dataset.head(10))\n",
    "display(dataset.tail(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4bfdb93bffcdb30a",
   "metadata": {},
   "source": [
    "## Step 3: Analyze the dataset\n",
    "\n",
    "As it may be seen in above cell, we have data about the house prices and their features, such as:\n",
    "* Price of the house.\n",
    "* Area,\n",
    "* Nr. of bathrooms,\n",
    "* Nr. of bedrooms,\n",
    "* Nr. of stories,\n",
    "* If the house is on main road or not,\n",
    "* If the house has a guest room or not,\n",
    "* If the house has a basement or not,\n",
    "* If the house has hot water heating or not,\n",
    "* If the house has an air conditioning or not,\n",
    "* Nr. of parking spaces,\n",
    "* If the house is in a preferred area or not,\n",
    "* If the house is furnished, semi-furnished or not furnished,\n",
    "\n",
    "Each of those features may be used to predict the price of the house, however, as it will be seen later, some features are more important than over, and if used not properly, they may lead to a worse model performance, as well as higher training time.\n",
    "\n",
    "Since we have the examples of houses alongside with their actual price, then it is safely to assume that I will apply Supervised Learning algorithm to train a model that will predict the price of the house based on the given features. Supervised Machine Learning - a type of Machine Learning that refers to algorithms that learn from labeled data (data that provides examples with correct answers), learns from being given the X and Y mapping, Input and Output mapping. [[1]](https://www.coursera.org/learn/machine-learning/lecture/s91wX/supervised-learning-part-1)\n",
    "\n",
    "At the same time, I was instructed to provide some Initial Assumptions on the dataset and the house prediction model that may be built on that dataset. So, I will start with that.\n",
    "Taken in the account the real life experience and the brief analysis of the dataset, I may suppose that:\n",
    "1. The price of the house is strongly influenced by the Area of the house, since this is the main criteria for the evaluation of the price of the house.\n",
    "2. The number of stories influences the actual area of the house, therefore it is a parameter that will formulate the Area and, therefore, the actual price.\n",
    "3. Houses in the preferred areas are more expensive than those in the non-preferred areas.\n",
    "4. Furnished houses are more expensive than semi-furnished and non-furnished houses.\n",
    "\n",
    "The Initial Assumptions on the house pricing:\n",
    "1. Main criteria in the price evaluation is the Area of the house.\n",
    "2. Houses that are furnished are more expensive than those that are not furnished.\n",
    "3. Houses with more rooms are more expensive.\n",
    "4. Houses with at least 1 parking space is more expensive than those without parking spaces."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3.1: Analyze the dimensionality of the dataset\n",
    "\n",
    "Dimensionality of the dataset is actually about the numbers of rows and columns in the dataset.\n",
    "* Number of rows - number of examples in the dataset.\n",
    "* Number of columns - number of features in the dataset, however since it is labeled dataset, 1 column from the total number of columns is the target column, that contains the Y values, the actual prices of the houses. Other columns contain different features (input variables or X values) that influences the target values."
   ],
   "id": "94f1778979a9db19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Dimensionality of the dataset: {dataset.shape}\")\n",
    "print(f\"Number of rows (training examples): {dataset.shape[0]}\")\n",
    "print(f\"Number of columns (X - nr. of features vectors, Y - targets vector): {dataset.shape[1]}\")"
   ],
   "id": "b20bb4658dcbb69e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this case, we have 545 examples of houses, their input variables and the respective output variable. Alongside with this number, we have 13 columns, that contains actually 12 features and 1 target column.\n",
    "\n",
    "In the context of ML practices, I will denote the above numbers with the following notation:\n",
    "$$\n",
    "\\begin{split}\n",
    "m = 545 \\\\\n",
    "|\\vec{X}| = 12 \\text{ (for a single training example)}\\\\\n",
    "|\\vec{Y}| = 1 \\text{ (for a single training example)}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "Beside that, in case I want to refer to a specific training example, I will use the following notation:\n",
    "$$\n",
    "\\begin{split}\n",
    "i = 0 \\text{ ($i^{th}$ training example)}\\\\\n",
    "(X^{(0)}, Y^{(0)}) = ([7420,4,2,3,yes,no,no,no,yes,2,yes,furnished], [13300000]) \\\\\n",
    "\\end{split}\n",
    "$$"
   ],
   "id": "a51c6ce466b97521"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3.2 - Analyze the dataset for inconsistencies\n",
    "\n",
    "In the next step, I am going to explore the dataset on inconsistencies, such as missing values, and analyze the types I have in it."
   ],
   "id": "48eff052b22cca03"
  },
  {
   "cell_type": "code",
   "id": "85d09b1865a62fe7",
   "metadata": {},
   "source": [
    "print(\"Information:\")\n",
    "dataset.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b80afa3c9e62364",
   "metadata": {},
   "source": [
    "As it may be seen, the dataset contains 545 entries, as I already found out, and 13 columns. The columns are of different datatypes, such as:\n",
    "* int64 - integer,\n",
    "* object - I can deduce from the brief analysis on the head and tail of the dataframe, that this is a string datatype, that contains categorical data:\n",
    "    * yes, no - by analogy, True/False values, or 1/0 values,\n",
    "    * furnished, semi-furnished, not furnished - different categories of the same feature (furnishingstatus).\n",
    "\n",
    "At the same time, it may be seen that there are no missing (NULL) values, but to be sure, I will check it again.\n",
    "\n",
    "I used the isnull() method, that will return a DataFrame with boolean values, where True value represents the missing value, and False value represents the existing value. By summing the values of the DataFrame, I will get the number of missing values in each column."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Missing values:\")\n",
    "print(dataset.isnull().sum())"
   ],
   "id": "b4d6612a8b5461f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As it may be seen, no missing values in the dataset, which is a good thing, since there are 3 ways I know to handle missing values:\n",
    "* Drop the rows with missing values - this is an acceptable approach if the number of missing values is small, and the dataset is large enough to be able to compensate the loss of those examples with very large total number of examples.\n",
    "* Interpolate the missing values - this is a good approach if the number of missing values is small, and the dataset is small. It uses the values of the neighboring examples to fill the missing values.\n",
    "* Fill the missing values with the mean, median or mode of the column - this is a good approach if the number of missing values is large, and the dataset is large enough to be able to provide sufficient points that will be used to calculate the mean, median or mode of the column, offering less biased values for those missing ones."
   ],
   "id": "9eb5f0b618e1a405"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next step is to find out the duplicate examples in the dataset. However, I am not sure if the case I analyze (house price prediction) is one that does not allow duplicate values, since it may probabilistically 2 house may be similar and have similar prices (new houses in the same neighborhood, for example). However, I will still check for duplicates in the dataset, just to adhere to general good practices in data analysis. In the cell below, I use the duplicated() method with parameter keep set to False, which will mark all the duplicate rows. It will return the dataframe with rows and boolean that will show if the row is a duplicate. By passing this Series (obtained after duplicated() method) to the dataset, I will get only the rows that are duplicates, playing the role of a filter.",
   "id": "cf58e511ba324430"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(dataset[dataset.duplicated(keep=False)])",
   "id": "2a58e3929c56f9e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As it may be seen, there are no duplicate examples in the dataset.",
   "id": "5fdcadb8614522f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3.3 - Analyze the dataset for categorical data\n",
    "\n",
    "The next step is to convert the categorical data into binary values (0/1) in order to be able to use them in the model training. At the same time, they will be already in a normalized range of values. For this, I will be using One Hot Encoding, which is a process of converting categorical data into binary values, where each category is represented by a binary value. For example, if we have 3 categories, we will have 3 columns, each representing one category, and the value of the column will be 1 if the category is present in the example, and 0 otherwise. This is a good approach to handle categorical data, since it will not introduce any bias in the model training, and it will not affect the model performance [[2](https://www.geeksforgeeks.org/ml-one-hot-encoding/)]. However, this approach may be applied to data that has not a clear order, like binary values (mainroad, guestroom etc.), where the order of the values does not matter. At the same time, I might use another Encoding method - Ordinal Encoding - a process of converting categorical data into integer values, where the order of the values matters [[3](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/)]. In my case, I will use this approach for furnishingstatus feature, since it has a clear order of the values:\n",
    "$$\n",
    "\\text{furnished} > \\text{semi-furnished} > \\text{not furnished}\n",
    "$$\n",
    "\n",
    "I used select_dtypes() method to select the columns with specified datatypes, which returns a DataFrame with the columns that have the specified datatypes. In this case, I used include parameter with value 'object', which will return the columns that have object datatype, which, since I saw their values in the dataset and can say that it is the case, are categorical columns. I then get the Index object, that contains the names of the columns, and drop the furnishingstatus column from it, since I will use Ordinal Encoding for it. The rest of the columns will be used for One Hot Encoding."
   ],
   "id": "a173c8585b76340d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nominal_columns = dataset.select_dtypes(include=['object']).columns.drop('furnishingstatus')\n",
    "# nominal_categorical_rows = dataset.select_dtypes(include=['object']).drop('furnishingstatus', axis=1)\n",
    "ordered_categorical_columns = dataset.select_dtypes(include=['object']).columns.drop(nominal_columns)\n",
    "\n",
    "print(f\"Nominal columns:\\n{nominal_columns}\")\n",
    "print(f\"Ordinal columns:\\n{ordered_categorical_columns}\")"
   ],
   "id": "cbcb3d563d0515f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the above cell, I extract all the categorical fields, and specify which is nominal (with no specific order between the actual values) and ordered (with a specific order between the actual values). The furnishingstatus is the only ordered categorical field, and I will use Ordinal Encoding for it, while for the rest of the fields, I will use One-Hot Encoding.\n",
    "\n",
    "For the One Hot Encoding, I want to be sure that there are only 2 values for the nominal columns (Yes/No). For this, I iterate over each nominal column and use a dictionary to store the mapping between the actual column and the array of unique values in that column. I then print them in order to identify if there are no inconsistencies in the nominal values."
   ],
   "id": "36034605aeb3fb31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_nominal_values = {col: dataset[col].unique() for col in nominal_columns}\n",
    "for col, values in unique_nominal_values.items():\n",
    "    print(f\"{col}: {values}\")\n"
   ],
   "id": "dd9420dbe4c0a1d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "I decided to use get_dummies() from Pandas, which will convert the specified columns from dataset into specified datatype (in my case, integer), and, since there are only Yes/No values, therefore the output will be a binary one (0/1). However, there is another way to encode the binary values, and that is by using Label Encoding, from Scikit-learn library. It will take the values from a column and will encode them into encoded labels."
   ],
   "id": "4a420155ca4cdd6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# dataset = pd.get_dummies(dataset, columns=nominal_columns, drop_first=True, dtype=int)",
   "id": "683af77415353852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for column in nominal_columns:\n",
    "    dataset[column] = label_encoder.fit_transform(dataset[column])"
   ],
   "id": "332dadedfc14f0e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"One Hot Encoded dataset:\")\n",
    "display(dataset.head(10))"
   ],
   "id": "556e64f3c60a4e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the first encoding, I proceed with the Ordinal Encoding. First, I check if the column (furnishingstatus) has only 3 unique values - unfurnished, semi-furnished and furnished.",
   "id": "be500d74ed67612e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_ordinal_categorical_values = {ordinal_column: dataset[ordinal_column].unique() for ordinal_column in ordered_categorical_columns}\n",
    "for ordinal_column, values in unique_ordinal_categorical_values.items():\n",
    "    print(f\"{ordinal_column}: {values}\")"
   ],
   "id": "ea490c9d19c0aa2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a result, the column that holds ordinal data actually holds only 3 unique values, that may be represented as this schema:\n",
    "$$\n",
    "\\text{not furnished} < \\text{semi-furnished} < \\text{furnished}\n",
    "$$\n",
    "\n",
    "I used a dictionary to map the actual values to the integer values and used map() from Pandas to apply the mapping to the column.\n",
    "\n"
   ],
   "id": "fe2111947db11a03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "#\n",
    "# furnishing_statuses = unique_ordinal_categorical_values['furnishingstatus'].tolist()\n",
    "# furnishing_statuses.reverse()\n",
    "# furnishing_statuses = np.array([furnishing_statuses]).reshape(-1, 1)\n",
    "# print(furnishing_statuses)\n",
    "# encoder = OrdinalEncoder()\n",
    "#\n",
    "# encoded = encoder.fit_transform(furnishing_statuses)\n",
    "# print(encoded)\n",
    "# dataset['furnishingstatus'] = encoder.fit_transform(dataset[['furnishingstatus']])"
   ],
   "id": "a092f5d896afd62a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp_dict = {\n",
    "    'unfurnished': 0,\n",
    "    'semi-furnished': 1,\n",
    "    'furnished': 2\n",
    "}\n",
    "\n",
    "dataset['furnishingstatus'] = dataset.furnishingstatus.map(temp_dict)"
   ],
   "id": "fbde1c2611c211b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset.head(10)",
   "id": "9dfa5485f4c2f674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the encoding, I got the column furnishingstatus with values between 0 and 2.",
   "id": "44b6f1652b78b4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3.4 - Visualization of the dataset\n",
    "\n",
    "In this step, I will focus on visual analysis using graphs and plots to better understand the dataset and the relationships between features. I will use the Seaborn library, which is a Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics [[4](https://seaborn.pydata.org/)]."
   ],
   "id": "e5f114b250937951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First of all, I want to plot the data separately for each feature, in order to see the distribution of the data. For this, I will use the pairplot() method from Seaborn library, which will plot the data for each feature in the dataset, and the relationship between them. The diagonal of the plot will contain the distribution of the data for each feature, while the other plots will contain the relationship between the features.",
   "id": "6a10cbfc0f66c1ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# sns.pairplot(dataset, height=2.5, hue='price', palette='husl')",
   "id": "565cd59604be826b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next plot I want to analyze is the correlation matrix between each feature in the dataset. The correlation matrix shows the relationship and correlation coefficients between each variable [[5](https://www.displayr.com/what-is-a-correlation-matrix/)]. The result value is in range:\n",
    "$$\n",
    "\\begin{split}\n",
    "-1 \\leq r \\leq 1 \\text{, where:} \\\\\n",
    "-1 \\text{ - negative correlation} \\\\\n",
    "0 \\text{ - no correlation} \\\\\n",
    "1 \\text{ - positive correlation, and} \\\\\n",
    "r \\in (0, 1] \\text{ - the corresponding variables are directly proportional} \\\\\n",
    "r \\in [-1, 0) \\text{ - the corresponding variables are inversely proportional} \\\\\n",
    "\\end{split}\n",
    "$$"
   ],
   "id": "2c8f482f424a2eb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(dataset.corr(), annot=True, linewidths=0.5, cbar=True)\n",
    "plt.show()"
   ],
   "id": "1e4f9e44053b9983",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As it may be noticed, in the context of my dataset, the 3 top features that have the greatest impact on the price of the house are:\n",
    "* area - the area of the house (.54),\n",
    "* bathrooms - the number of bathrooms in the house (.52),\n",
    "* airconditioning - if the house has an air conditioning or not (.45).\n",
    "and the 3 features that have the least impact on the price of the house are:\n",
    "* hotwaterheating - if the house has hot water heating or not (.09),\n",
    "* basement - if the house has a basement or not (.19),\n",
    "* guestroom - if the house has a guest room or not (.26).\n",
    "\n",
    "Somewhat, the main assumptions from the beginning were correct, specifically the fact that main criteria for price setting is area of the house, however, the furnishing of the house is in the middle range of the features that influence the price of the house, but still influence the price by increasing it. At the same time, the availability of a parking area around the house also is influencing the price of the house, in a greater manner than the furnishing of the house. At the same time, the fact that house has more rooms, also greatly influence the price of the house, which is logical, since the area of the house is directly proportional to the number of rooms in the house. Whatsoever, the initial assumptions were quite correct, and the analysis of the dataset confirmed them."
   ],
   "id": "b167d414da93f005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, I will construct the countplots for each categorical data. A countplot basically counts the categories and returns a count of their occurrences [[7](https://www.geeksforgeeks.org/seaborn-categorical-plots/?ref=ml_lbp)]. This will help me to understand the distribution of the data in the dataset.",
   "id": "f1ac2ae64b983697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='mainroad', data=dataset)"
   ],
   "id": "f210750260e0991a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='bedrooms', data=dataset)"
   ],
   "id": "cee1e4c31dc155f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='bathrooms', data=dataset)"
   ],
   "id": "18dc4de3d890aeb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='stories', data=dataset)"
   ],
   "id": "c1671b52858dcd07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='parking', data=dataset)"
   ],
   "id": "5275927f77e725ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x=dataset[\"furnishingstatus\"].map({0: 'unfurnished', 1: 'semi-furnished', 2: 'furnished'}), data=dataset)"
   ],
   "id": "ec8347915551963",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='guestroom', data=dataset)"
   ],
   "id": "7f25f7ac8bc0b532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='basement', data=dataset)"
   ],
   "id": "5d5083e7e80ed2a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='hotwaterheating', data=dataset)"
   ],
   "id": "7b799632a93b2430",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='airconditioning', data=dataset)"
   ],
   "id": "74363b86105ceb1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x='prefarea', data=dataset)"
   ],
   "id": "1e94f04eab833c3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a result of the above plots, I may conclude the following. Most of the houses:\n",
    "* Are on the **main** road,\n",
    "* have **3** bedrooms, less likely to have **2** or **4**, and very unlikely to have **1**, **5**, **6** bedrooms,\n",
    "* have **1** bathroom, less likely to have **2** and very unlikely to have **3** and **4** bathrooms,\n",
    "* have **2** or **1** story, less likely to have **3** and **4** stories,\n",
    "* have **no** parking space, less likely to have **1** or **2** parking spaces, and very unlikely to have **3** parking spaces,\n",
    "* are **semi-furnished**, slightly less likely to be **unfurnished** and less likely to be **furnished**,\n",
    "* have **no** guest room, less likely to have a guest room,\n",
    "* have **no** basement, less likely to have a basement,\n",
    "* have **no** hot water heating, very unlikely to have hot water heating,\n",
    "* have **no** air conditioning, less likely to have air conditioning,\n",
    "* are **not** in the preferred area, less likely to be in the preferred area."
   ],
   "id": "ab499b7fb29bcf69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, I will plot the boxplot for each categorical feature and the price of the house. A boxplot uses boxes and lines to depict the distributions of one or more groups of numeric data. Box limits indicate the range of the central 50% of the data, with a central line marking the median value [[8](https://www.atlassian.com/data/charts/box-plot-complete-guide#:~:text=A%20box%20plot%20(aka%20box,line%20marking%20the%20median%20value.)].",
   "id": "29300c190efb5021"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='bedrooms', y='price', data=dataset)"
   ],
   "id": "7a098c3e41c4b623",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='bathrooms', y='price', data=dataset)"
   ],
   "id": "bab755485d1dd427",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='stories', y='price', data=dataset)"
   ],
   "id": "9410982676493930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='mainroad', y='price', data=dataset)"
   ],
   "id": "ef24ec0101862379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='guestroom', y='price', data=dataset)"
   ],
   "id": "c0c23114224b7644",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='basement', y='price', data=dataset)"
   ],
   "id": "3800e828f4e45ba0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='hotwaterheating', y='price', data=dataset)"
   ],
   "id": "bfff52ccc2a673f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='airconditioning', y='price', data=dataset)"
   ],
   "id": "4285bc264e9f7d20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='parking', y='price', data=dataset)"
   ],
   "id": "50ac74a82082b180",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x='prefarea', y='price', data=dataset)"
   ],
   "id": "279cc0b8d8b3d9bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(x=dataset['furnishingstatus'].map({0: 'unfurnished', 1: 'semi-furnished', 2: 'furnished'}), y='price', data=dataset)"
   ],
   "id": "d5b5cd04811b44fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From the above boxplots, I can deduce that there are multiple outliers in the dataset for each feature, for example, there are a lot of houses that are semi-furnished and have a price higher than the average price of the house. That fact I will take in consideration in the next step when I will normalize the dataset. However, I observed that mainly the distribution of those outliers still reassemble the general distribution of data based on price, furnished houses are more expensive that unfurnished, and the outliers in this category still reassemble that distribution, which is not really a bad thing, since the model will still be able to learn the general distribution of the data.",
   "id": "27842c3e4210794e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, I will plot the line plots for each numerical feature. A line chart uses points connected by line segments from left to right to demonstrate changes in value. The horizontal axis depicts a continuous progression, often that of time, while the vertical axis reports values for a metric of interest across that progression [[9](https://www.atlassian.com/data/charts/line-chart-complete-guide)].",
   "id": "2706c2daf243401e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x='area', y='price', data=dataset)"
   ],
   "id": "6044dc9d6a208a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset)"
   ],
   "id": "b1929bad71272f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From the above plots, I deduce that the price is directly proportional to the area of the house. However, I am also interested in the correspondence with other features of these 2 numerical data. For this, I will use the scatterplot, that will show the relationship between the area and the price of the house, and at the same time, I will use the hue parameter to color the points based on other categorical features, since it may be used generally for the analysis of the relationship between features [[9](https://www.atlassian.com/data/charts/what-is-a-scatter-plot#:~:text=What%20is%20a%20scatter%20plot,to%20observe%20relationships%20between%20variables.)].",
   "id": "17215e81e52140cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='bedrooms', palette='husl')"
   ],
   "id": "97edea8a00fdcc3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='bathrooms', palette='husl')"
   ],
   "id": "1a376b0e2fbc804a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='stories', palette='husl')"
   ],
   "id": "87928e46e208d18f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='mainroad', palette='husl')"
   ],
   "id": "10836919434357b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='guestroom', palette='husl')"
   ],
   "id": "3a87a741c6532c0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='basement', palette='husl')"
   ],
   "id": "c487f9e1f0c7d764",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='hotwaterheating', palette='husl')"
   ],
   "id": "f39702ec3178c9c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='airconditioning', palette='husl')"
   ],
   "id": "e0d747b58607b215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='parking', palette='husl')"
   ],
   "id": "a2aefe384724402b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue='prefarea', palette='husl')"
   ],
   "id": "ce1b399859aa70d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='area', y='price', data=dataset, hue=dataset['furnishingstatus'].map({0: \"unfurnished\", 1: \"semifurnished\", 2: \"furnished\"}), palette='husl')"
   ],
   "id": "3037a6e5f68577de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the above scatterplots, I can deduce the following:\n",
    "* Houses with greater area and more bedrooms are more expensive,\n",
    "* Houses with greater area and more bathrooms are **clearly** more expensive, which confirms the correlation matrix,\n",
    "* Houses with greater area and more stories are not always more expensive. In this case, the stories do not influence so much the price of the house since it has a very low (close to 0) correlation with the area of the house, although it has a relatively high correlation with the price of the house (.42),\n",
    "* Houses with greater area of the house and that are placed at the main road are more expensive. Also, houses with less area are usually not near the main road. Also, this is not conclusive since there area lot less houses near the main road in the dataset.\n",
    "* Most of the houses do not have a guestroom. The impact on the price is .26 and on the area - .14, so not a very big influence on the graph as well. However, some houses with guestroom have greater area and price, but taking in count that there are not so much houses with guestroom in the dataset, it is not really conclusive.\n",
    "* Most of the houses do not have a basement. The impact on the price is .19 and on the area - .047, so not a big influence on the graph, as in the previous case.\n",
    "* Majority of the houses do not have a hot water heating system, the influence on the price is minor - .093 and on the area is also appreciatively close to 0 - -.0092. Therefore, this feature will not affect the price practically at all and will not be a very useful feature in the model training.\n",
    "* Houses with air conditioning are more pricey, the impact on the price and area are both relatively high (on price - .45 and on area - .22). I suppose its due to the fact that houses with greater area are in more need of air conditioning in order to maintain a stable temperature in the whole building.\n",
    "* Houses with at least 1 parking spot are more expensive than without parking area. Also, the impact on the price is high - .38, and on the area is also high - .35. Even though the data about houses without a parking spot are more than with parking spot(s), it suggests that the parking spot also plays a role in the price of the house.\n",
    "* Houses in preferred areas are more expensive, that placement of the house in a preferred area has an influence of .33 on the price and on the area - .23, I assume this is because preferred areas are somewhere that permits houses with more area and access to main road.\n",
    "* Houses that are furnished are more expensive."
   ],
   "id": "49983e06acadc57f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3.4 - Normalization of the dataset\n",
    "\n"
   ],
   "id": "fab2502eab5ca62a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the start, I need to separate the dataset into features and target values, in conformity with the previously defined relationship:\n",
    "$$\n",
    "\\begin{split}\n",
    "m = 545 \\\\\n",
    "|\\vec{X}| = 12 \\text{ (for a single training example)}\\\\\n",
    "|\\vec{Y}| = 1 \\text{ (for a single training example)}\\\\\n",
    "\\end{split}\n",
    "$$"
   ],
   "id": "b6303a103667ea03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_features = dataset.drop('price', axis=1)\n",
    "X_features"
   ],
   "id": "bfe14575d7f5de0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_target = dataset['price']\n",
    "y_target"
   ],
   "id": "e55f549621ac9136",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then, I need to split both new datasets into the following groups of sets:\n",
    "* **Training Set** - used to train the model\n",
    "* **Cross Validation Set** - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset. In this case, I will try different Polynomial Regression configurations.\n",
    "* **Test Set** - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models.\n",
    "\n",
    "However, in this case, since I want to use a Linear Regression Model, and I do not aim to test different models and check what model suits better, I will just use Training set and Test set.\n"
   ],
   "id": "e410c3ccb2f5a976"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# X_train, X_rest, y_train, y_rest = train_test_split(X_features, y_target, test_size=0.4, random_state=1)\n",
    "# X_cv, X_test, y_cv, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=1)\n",
    "# del X_rest, y_rest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=1)"
   ],
   "id": "220cbcb92f96efeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train.head(5)",
   "id": "dab6aca5b8a7edd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train.head(5)",
   "id": "31a0498b3d0ccc44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# X_cv.head(5)",
   "id": "6f0e8ef39783914b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# y_cv.head(5)",
   "id": "f77ad4f2278c2f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_test.head(5)",
   "id": "5616cf674caa3e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_test.head(5)",
   "id": "241d7589ea7cfe8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next step, I will perform the normalization/scaling the features. I decided to use the Z-Score normalization - the process of normalizing every value in a dataset such that the mean of all the values is 0 and the standard deviation is 1 [[10](https://www.statology.org/z-score-normalization/#:~:text=Z%2Dscore%20normalization%20refers%20to,the%20standard%20deviation%20is%201.&text=where%3A,%CE%BC%3A%20Mean%20of%20data)]:\n",
    "$$\n",
    "\\begin{split}\n",
    "Z = \\frac{x_i - \\mu_i}{\\sigma_i} \\text{, i = }i^{th}\\text{ training example} \\\\\n",
    "\\mu_j = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m x_j^{(i)} \\\\\n",
    "\\sigma_j = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m (x_j^{(i)} - \\mu_j)^2\n",
    "\\end{split}\n",
    "$$\n",
    "In this case, Z-Score normalization operates on the 2 main concepts:\n",
    "* Mean - a quantity representing the \"center\" of a collection of numbers and is intermediate to the extreme values of the set of numbers [[11](https://en.wikipedia.org/wiki/Mean)].\n",
    "* Standard deviation - in statistics, the standard deviation is a measure of the amount of variation of the values of a variable about its mean [[12](https://en.wikipedia.org/wiki/Standard_deviation)]."
   ],
   "id": "b535a57ea2172294"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# y_train_reshaped = y_train.values.reshape(-1, 1)\n",
    "# y_train_reshaped"
   ],
   "id": "24d1c1738a4eddf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# y_test_reshaped = y_test.values.reshape(-1, 1)\n",
    "# y_test_reshaped"
   ],
   "id": "969d5e337798c4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# y_standard_scaler = StandardScaler()\n",
    "#\n",
    "# y_train_scaled = y_standard_scaler.fit_transform(y_train_reshaped)\n",
    "# y_test_scaled = y_standard_scaler.transform(y_test_reshaped)"
   ],
   "id": "175d61057701aa3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# pd.DataFrame(y_train_scaled, columns=[\"price\"])",
   "id": "d6196e48c973fe83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# pd.DataFrame(y_train_scaled, columns=[\"price\"]).describe()",
   "id": "e790409e1bb4c158",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# pd.DataFrame(y_test_scaled, columns=[\"price\"])",
   "id": "118ce9e076ff95e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# pd.DataFrame(y_test_scaled, columns=[\"price\"]).describe()",
   "id": "330cb8a2f090b2eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_standard_scaler = MinMaxScaler()\n",
    "X_train_scaled = X_standard_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_standard_scaler.transform(X_test)"
   ],
   "id": "2d5bdee1f956a777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "column_names = X_train.columns\n",
    "pd.DataFrame(X_train_scaled, columns=column_names)"
   ],
   "id": "91d6e2444a8c687",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.DataFrame(X_train_scaled, columns=column_names).describe()",
   "id": "54686213b454d556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "column_names = X_test.columns\n",
    "pd.DataFrame(X_test_scaled, columns=column_names)"
   ],
   "id": "807c4eec90fcac58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.DataFrame(X_test_scaled, columns=column_names).describe()",
   "id": "66f196a3f5428ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As it may be noticed, in the above description of the dataframe that is scaled, the mean value is close to 0, and the standard deviation is close to 1, which is expected from the Z-Score Normalization.",
   "id": "9a480cf57a1eafaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the next step, I am using Scikit-Learn Linear Regression model, that will be trained on my X_train_scaled and y_train values in order to obtain a model that has a set of weights for each feature and that will be able to predict new prices.",
   "id": "e3d54ad8b14791e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train)"
   ],
   "id": "8179d23f52d52a10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For test purposes, I will be using Mean Squared Error formula from Scikit-Learn library:\n",
    "$$\n",
    "\\begin{split}\n",
    "MSE = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m (y_i - \\hat{y}_i)^2\n",
    "\\end{split}\n",
    "$$\n",
    "MSE will measure the average of the squares of the errors [[13](https://en.wikipedia.org/wiki/Mean_squared_error)],\n",
    "and R2 score, that will show the accuracy of your model on a percentage scale [[14](https://www.freecodecamp.org/news/evaluation-metrics-for-regression-problems-machine-learning/)]:\n",
    "$$\n",
    "\\begin{split}\n",
    "R^2 = 1 - \\frac{RSS}{TSS} \\text{, where:} \\\\\n",
    "R^2\\text{ - coefficient of determination,} \\\\\n",
    "RSS\\text{ - sum of squares of residuals,} \\\\\n",
    "TSS\\text{ - total sum of square,} \\\\\n",
    "RSS = \\displaystyle\\sum_{i=1}^m (y_i - \\hat{y}_i)^2 \\\\\n",
    "TSS = \\displaystyle\\sum_{i=1}^m (y_i - \\bar{y})^2 \\\\\n",
    "y_i \\text{ - the actual value,} \\\\\n",
    "\\hat{y}_i \\text{ - the predicted value,} \\\\\n",
    "\\bar{y}_i \\text{ - the mean value of the variable} \\\\\n",
    "\\end{split}\n",
    "$$"
   ],
   "id": "8ffc786c1a84470e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yhat_training = linear_model.predict(X_train_scaled)\n",
    "# print(f\"Training MSE: {mean_squared_error(y_standard_scaler.inverse_transform(y_train_scaled), y_standard_scaler.inverse_transform(yhat_training)) / 2}\")\n",
    "# print(f\"Training R2 Score: {r2_score(y_standard_scaler.inverse_transform(y_train_scaled), y_standard_scaler.inverse_transform(yhat_training))}\")\n",
    "\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, yhat_training) / 2}\")\n",
    "print(f\"Training R2 Score: {r2_score(y_train, yhat_training)}\")\n"
   ],
   "id": "37bfeac421acd486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yhat_test = linear_model.predict(X_test_scaled)\n",
    "# print(f\"Test MSE: {mean_squared_error(y_standard_scaler.inverse_transform(y_test_scaled), y_standard_scaler.inverse_transform(yhat_test)) / 2}\")\n",
    "# print(f\"Test R2 Score: {r2_score(y_standard_scaler.inverse_transform(y_test_scaled), y_standard_scaler.inverse_transform(yhat_test))}\")\n",
    "\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, yhat_test) / 2}\")\n",
    "print(f\"Test R2 Score: {r2_score(y_test, yhat_test)}\")"
   ],
   "id": "282e872b85b96675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As it may be noticed from the above outputs, the accuracy of the model on training set is:\n",
    "$$\n",
    "R_{training}^2 \\approx 68 \\%\n",
    "$$\n",
    "and on the test set:\n",
    "$$\n",
    "R_{test}^2 \\approx 66 \\%\n",
    "$$\n",
    "\n",
    "That is not a very good result, but since I am not so proficient in Machine Learning field, I can say that I tried my best to understand fully the problem I had."
   ],
   "id": "df6fe14139eaf1ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=y_train, y=yhat_training, line_kws={'color':'r'})\n",
    "plt.title('Actual vs Predicted House Prices (Linear Regression Model) - Training', fontsize=14)\n",
    "plt.xlabel('Actual Prices', fontsize=12)\n",
    "plt.ylabel('Predicted Prices', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "54ad24295af7b26a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=y_test, y=yhat_test, line_kws={'color':'r'})\n",
    "plt.title('Actual vs Predicted House Prices (Linear Regression Model) - Test', fontsize=14)\n",
    "plt.xlabel('Actual Prices', fontsize=12)\n",
    "plt.ylabel('Predicted Prices', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "60ade183155ffcdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coefficients = linear_model.coef_\n",
    "intercept = linear_model.intercept_\n",
    "\n",
    "fitted_function_parts = [f\"{coefficient} * {feature}\" for coefficient, feature in zip(coefficients, X_features.columns)]\n",
    "fitted_equation = \" + \".join(fitted_function_parts)\n",
    "\n",
    "print(f\"Fitted equation: y = {intercept} + \" + fitted_equation)"
   ],
   "id": "91ee89cefaafb676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The obtained function is the following:\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = 1720448.0289697219 + 3572041.4631314613 * \\text{area} + 235994.3136255322 * \\text{bedrooms} + 3060519.72972627 * \\text{bathrooms} + 1577336.8900554231 * \\text{stories} + 476712.98481254204 * \\text{mainroad} + 275668.9507600057 * \\text{guestroom} + 557956.2675518387 * \\text{basement} + 822714.5575357171 * \\text{hotwaterheating} + 608777.0120531 * \\text{airconditioning} + 824209.3499264843 * \\text{parking} + 500668.0329551726 * \\text{prefarea} + 404925.3398917678 * \\text{furnishingstatus}\n",
    "\\end{equation}\n",
    "$$"
   ],
   "id": "fe8ada1e225d980f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "At the final of this task, I can conclude that I tried to adhere to general practices when it comes to training a Linear Regression Machine Learning Model, alongside with the feature scaling and dataset analysis. However, I think that there are a lot of mistakes I made due to my lack of knowledge the domain and I look forward to new knowledge so that I can optimize the models in the future.",
   "id": "cb2fc6bb67a76ecf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
